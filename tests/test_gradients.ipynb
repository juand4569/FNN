{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "667edd08",
   "metadata": {},
   "outputs": [],
   "source": [
    "\"\"\"\n",
    "Notebook para verificar que backpropagation está bien implementado\n",
    "usando gradient checking numérico\n",
    "\"\"\"\n",
    "import sys\n",
    "sys.path.append('../')\n",
    "\n",
    "import numpy as np\n",
    "from src.neural_network import NeuralNetwork\n",
    "from src.utils import one_hot_encode\n",
    "\n",
    "def numerical_gradient(model, X, y, epsilon=1e-7):\n",
    "    \"\"\"\n",
    "    Calcula gradientes numéricamente para verificar backprop\n",
    "    \"\"\"\n",
    "    params = model.get_params()\n",
    "    numerical_grads = []\n",
    "    \n",
    "    for param in params:\n",
    "        grad = np.zeros_like(param)\n",
    "        \n",
    "        # Iterar sobre cada elemento del parámetro\n",
    "        it = np.nditer(param, flags=['multi_index'])\n",
    "        while not it.finished:\n",
    "            idx = it.multi_index\n",
    "            old_value = param[idx]\n",
    "            \n",
    "            # f(x + epsilon)\n",
    "            param[idx] = old_value + epsilon\n",
    "            y_pred_plus = model.forward(X)\n",
    "            loss_plus = model.compute_loss(y_pred_plus, y)\n",
    "            \n",
    "            # f(x - epsilon)\n",
    "            param[idx] = old_value - epsilon\n",
    "            y_pred_minus = model.forward(X)\n",
    "            loss_minus = model.compute_loss(y_pred_minus, y)\n",
    "            \n",
    "            # Gradiente numérico: (f(x+eps) - f(x-eps)) / (2*eps)\n",
    "            grad[idx] = (loss_plus - loss_minus) / (2 * epsilon)\n",
    "            \n",
    "            # Restaurar valor\n",
    "            param[idx] = old_value\n",
    "            it.iternext()\n",
    "        \n",
    "        numerical_grads.append(grad)\n",
    "    \n",
    "    return numerical_grads\n",
    "\n",
    "def gradient_check(model, X, y, epsilon=1e-7, threshold=1e-7):\n",
    "    \"\"\"\n",
    "    Compara gradientes analíticos (backprop) vs numéricos\n",
    "    \"\"\"\n",
    "    # Forward y backward\n",
    "    y_pred = model.forward(X)\n",
    "    model.backward(y)\n",
    "    analytical_grads = model.get_grads()\n",
    "    \n",
    "    # Gradientes numéricos\n",
    "    print(\"Calculando gradientes numéricos (puede tardar)...\")\n",
    "    numerical_grads = numerical_gradient(model, X, y, epsilon)\n",
    "    \n",
    "    # Comparar\n",
    "    print(\"\\nComparación de gradientes:\")\n",
    "    print(\"=\"*60)\n",
    "    \n",
    "    param_names = []\n",
    "    for i, layer in enumerate(model.layers):\n",
    "        param_names.extend([f\"Layer {i} W\", f\"Layer {i} b\"])\n",
    "    \n",
    "    all_close = True\n",
    "    for i, (name, analytical, numerical) in enumerate(\n",
    "        zip(param_names, analytical_grads, numerical_grads)):\n",
    "        \n",
    "        # Diferencia relativa\n",
    "        numerator = np.linalg.norm(analytical - numerical)\n",
    "        denominator = np.linalg.norm(analytical) + np.linalg.norm(numerical)\n",
    "        relative_error = numerator / (denominator + 1e-10)\n",
    "        \n",
    "        status = \"✓ OK\" if relative_error < threshold else \"✗ ERROR\"\n",
    "        print(f\"{name:20s} - Error relativo: {relative_error:.2e} {status}\")\n",
    "        \n",
    "        if relative_error >= threshold:\n",
    "            all_close = False\n",
    "    \n",
    "    print(\"=\"*60)\n",
    "    if all_close:\n",
    "        print(\"✓ Backpropagation implementado correctamente\")\n",
    "    else:\n",
    "        print(\"✗ Hay errores en backpropagation\")\n",
    "    \n",
    "    return all_close\n",
    "\n",
    "# Test con XOR\n",
    "print(\"TEST 1: XOR\")\n",
    "X = np.array([[0,0],[0,1],[1,0],[1,1]])\n",
    "y = one_hot_encode(np.array([0, 1, 1, 0]), 2)\n",
    "\n",
    "architecture = [(2, None), (4, 'sigmoid'), (2, 'softmax')]\n",
    "model = NeuralNetwork(architecture)\n",
    "\n",
    "gradient_check(model, X, y)\n",
    "\n",
    "print(\"\\n\" + \"=\"*60 + \"\\n\")\n",
    "\n",
    "# Test con datos aleatorios más complejos\n",
    "print(\"TEST 2: Datos aleatorios (3 capas)\")\n",
    "np.random.seed(42)\n",
    "X = np.random.randn(10, 5)\n",
    "y = one_hot_encode(np.random.randint(0, 3, 10), 3)\n",
    "\n",
    "architecture = [(5, None), (8, 'relu'), (6, 'sigmoid'), (3, 'softmax')]\n",
    "model = NeuralNetwork(architecture)\n",
    "\n",
    "gradient_check(model, X, y)"
   ]
  }
 ],
 "metadata": {
  "language_info": {
   "name": "python"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
